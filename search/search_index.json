{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Analysis Correlation Engine - Alert Management System \u00b6 ACE2 is comprised of the Core and the Alert Management System. The AMS provides a web interface for analysts to interact with the alerts. Quick Start \u00b6 Your local system will need an entry in the hosts file to properly work with the AMS development environment. For Mac/Linux, this file is located at /etc/hosts . In Windows, this file is located at C:\\Windows\\System32\\drivers\\etc\\hosts . You will need to open Notepad or another text editor as an Administrator in order to edit the hosts file. Add the following entry to the file: 127.0.0.1 ace2-ams With your host file updated, you can use the helper script to reset and build the local AMS development environment that includes hot-reloading for both the frontend and backend: bin/reset-dev-container.sh After the containers are built and running, you can access the components using the following URLs: Frontend: http://ace2-ams:8080 Backend API Swagger documentation: http://localhost:7777/docs Backend API ReDoc documentation: http://localhost:7777/redoc Philosophy \u00b6 For a more in-depth understanding of the philosophy behind ACE, see the talk that John Davison gave on the development of the ACE tool set at BSides Cincinnati in 2015.","title":"Analysis Correlation Engine - Alert Management System"},{"location":"#analysis-correlation-engine-alert-management-system","text":"ACE2 is comprised of the Core and the Alert Management System. The AMS provides a web interface for analysts to interact with the alerts.","title":"Analysis Correlation Engine - Alert Management System"},{"location":"#quick-start","text":"Your local system will need an entry in the hosts file to properly work with the AMS development environment. For Mac/Linux, this file is located at /etc/hosts . In Windows, this file is located at C:\\Windows\\System32\\drivers\\etc\\hosts . You will need to open Notepad or another text editor as an Administrator in order to edit the hosts file. Add the following entry to the file: 127.0.0.1 ace2-ams With your host file updated, you can use the helper script to reset and build the local AMS development environment that includes hot-reloading for both the frontend and backend: bin/reset-dev-container.sh After the containers are built and running, you can access the components using the following URLs: Frontend: http://ace2-ams:8080 Backend API Swagger documentation: http://localhost:7777/docs Backend API ReDoc documentation: http://localhost:7777/redoc","title":"Quick Start"},{"location":"#philosophy","text":"For a more in-depth understanding of the philosophy behind ACE, see the talk that John Davison gave on the development of the ACE tool set at BSides Cincinnati in 2015.","title":"Philosophy"},{"location":"development/","text":"ACE2 AMS Development Guide \u00b6 Initial setup \u00b6 This project has VSCode devcontainer support to ensure that anyone working on the project does so in a consistent environment as well as follows the same formatting/styling guidelines. Required setup \u00b6 In order to work within the devcontainer, you will need the following installed on your system: Docker VSCode Remote Development VSCode extension pack NOTE: If you are developing in Windows, you will need to make sure that you have WSL 2 set up and properly configured with Docker. That is outside the scope of this documentation, but you can find steps here . Working in the VSCode devcontainer \u00b6 When you open the project in VSCode, it will detect the devcontainer configuration and prompt you to reopen it inside of the container: Once you choose the Reopen in Container option, VSCode will work on building the environment. Once it is complete, you can open a terminal within VSCode to interact with the application: Any work done on the application should be done through the devcontainer. If you make a change to the devcontainer configuration (found in the .devcontainer directory), you can rebuild the devcontainer by clicking on Dev Container: ACE2 AMS in the lower-left corner of VSCode and then selecting the Rebuild Container option in the menu that opens. Updating your hosts file \u00b6 Your local system will need an entry in the hosts file to properly work with the AMS development environment. For Mac/Linux, this file is located at /etc/hosts . In Windows, this file is located at C:\\Windows\\System32\\drivers\\etc\\hosts . You will need to open Notepad or another text editor as an Administrator in order to edit the hosts file. Add the following entry to the file: 127.0.0.1 ace2-ams Starting the application \u00b6 You can start the application using Docker containers so that it uses hot-reloading anytime you change a file: bin/reset-dev-container.sh This script will generate random passwords for the database user and the secret key used for JWTs. If you need to access these, you can view them in the $HOME/.ace2.env file, which configures the environment variables that will be loaded into the database container. Once the both the frontend and backend development environments are built and started, you can access the components: Frontend: http://ace2-ams:8080 Backend API Swagger documentation: http://localhost:7777/docs Backend API ReDoc documentation: http://localhost:7777/redoc Managing NPM packages \u00b6 You should not directly edit the dependencies or devDependencies inside of package.json or anything in package-lock.json . Any changes to packages should be performed via the npm command : Install new dependency package \u00b6 You would install a package like this if it is something the final compiled application needs: npm install <package> Install new dev dependency package \u00b6 You would install a package like this if it is only needed during development: npm install -D <package> Uninstall package \u00b6 You can uninstall/remove a package regardless of how it was installed by: npm uninstall <package> Running tests \u00b6 Backend \u00b6 The backend API has a suite of tests performed by Pytest that includes code coverage: bin/test-backend.sh You can run a specific portion of the tests using the same script: bin/test-backend.sh backend/app/tests/api/test_auth_validate.py Frontend \u00b6 This frontend has a suite of unit tests performed by Jest and end-to-end tests performed by Cypress . Unit tests \u00b6 You can execute the unit tests by running: bin/test-frontend-unit.sh End-to-end tests \u00b6 You can execute the end-to-end tests by running: bin/test-e2e.sh Test Runner \u00b6 Cypress also comes with an amazing Test Runner that lets you see and interact with the tests in your local web browser. This can be helpful when writing end-to-end tests to ensure they are working properly as well as any debugging you might need to do. However, this will need to be performed on your local system ouside of the containers. To do this, you will need to have Node.js 16 installed. Step 1: Install Cypress on your host system (this only needs to be done one time): npm install -g cypress@9.3.1 Step 2: Prep the application to run the tests in interactive mode: bin/test-interactive-e2e.sh Step 3: Open the Test Runner on your host system: cd cypress/ cypress open For more information on what you can do with the Test Runner, view the Test Runner documentation . Step 4: Disable testing mode once you are finished using the Test Runner: bin/disable-test-mode.sh","title":"ACE2 AMS Development Guide"},{"location":"development/#ace2-ams-development-guide","text":"","title":"ACE2 AMS Development Guide"},{"location":"development/#initial-setup","text":"This project has VSCode devcontainer support to ensure that anyone working on the project does so in a consistent environment as well as follows the same formatting/styling guidelines.","title":"Initial setup"},{"location":"development/#required-setup","text":"In order to work within the devcontainer, you will need the following installed on your system: Docker VSCode Remote Development VSCode extension pack NOTE: If you are developing in Windows, you will need to make sure that you have WSL 2 set up and properly configured with Docker. That is outside the scope of this documentation, but you can find steps here .","title":"Required setup"},{"location":"development/#working-in-the-vscode-devcontainer","text":"When you open the project in VSCode, it will detect the devcontainer configuration and prompt you to reopen it inside of the container: Once you choose the Reopen in Container option, VSCode will work on building the environment. Once it is complete, you can open a terminal within VSCode to interact with the application: Any work done on the application should be done through the devcontainer. If you make a change to the devcontainer configuration (found in the .devcontainer directory), you can rebuild the devcontainer by clicking on Dev Container: ACE2 AMS in the lower-left corner of VSCode and then selecting the Rebuild Container option in the menu that opens.","title":"Working in the VSCode devcontainer"},{"location":"development/#updating-your-hosts-file","text":"Your local system will need an entry in the hosts file to properly work with the AMS development environment. For Mac/Linux, this file is located at /etc/hosts . In Windows, this file is located at C:\\Windows\\System32\\drivers\\etc\\hosts . You will need to open Notepad or another text editor as an Administrator in order to edit the hosts file. Add the following entry to the file: 127.0.0.1 ace2-ams","title":"Updating your hosts file"},{"location":"development/#starting-the-application","text":"You can start the application using Docker containers so that it uses hot-reloading anytime you change a file: bin/reset-dev-container.sh This script will generate random passwords for the database user and the secret key used for JWTs. If you need to access these, you can view them in the $HOME/.ace2.env file, which configures the environment variables that will be loaded into the database container. Once the both the frontend and backend development environments are built and started, you can access the components: Frontend: http://ace2-ams:8080 Backend API Swagger documentation: http://localhost:7777/docs Backend API ReDoc documentation: http://localhost:7777/redoc","title":"Starting the application"},{"location":"development/#managing-npm-packages","text":"You should not directly edit the dependencies or devDependencies inside of package.json or anything in package-lock.json . Any changes to packages should be performed via the npm command :","title":"Managing NPM packages"},{"location":"development/#install-new-dependency-package","text":"You would install a package like this if it is something the final compiled application needs: npm install <package>","title":"Install new dependency package"},{"location":"development/#install-new-dev-dependency-package","text":"You would install a package like this if it is only needed during development: npm install -D <package>","title":"Install new dev dependency package"},{"location":"development/#uninstall-package","text":"You can uninstall/remove a package regardless of how it was installed by: npm uninstall <package>","title":"Uninstall package"},{"location":"development/#running-tests","text":"","title":"Running tests"},{"location":"development/#backend","text":"The backend API has a suite of tests performed by Pytest that includes code coverage: bin/test-backend.sh You can run a specific portion of the tests using the same script: bin/test-backend.sh backend/app/tests/api/test_auth_validate.py","title":"Backend"},{"location":"development/#frontend","text":"This frontend has a suite of unit tests performed by Jest and end-to-end tests performed by Cypress .","title":"Frontend"},{"location":"development/#unit-tests","text":"You can execute the unit tests by running: bin/test-frontend-unit.sh","title":"Unit tests"},{"location":"development/#end-to-end-tests","text":"You can execute the end-to-end tests by running: bin/test-e2e.sh","title":"End-to-end tests"},{"location":"development/#test-runner","text":"Cypress also comes with an amazing Test Runner that lets you see and interact with the tests in your local web browser. This can be helpful when writing end-to-end tests to ensure they are working properly as well as any debugging you might need to do. However, this will need to be performed on your local system ouside of the containers. To do this, you will need to have Node.js 16 installed. Step 1: Install Cypress on your host system (this only needs to be done one time): npm install -g cypress@9.3.1 Step 2: Prep the application to run the tests in interactive mode: bin/test-interactive-e2e.sh Step 3: Open the Test Runner on your host system: cd cypress/ cypress open For more information on what you can do with the Test Runner, view the Test Runner documentation . Step 4: Disable testing mode once you are finished using the Test Runner: bin/disable-test-mode.sh","title":"Test Runner"},{"location":"development/backend/database/","text":"Database \u00b6 Schema definitions \u00b6 The database tables used by the FastAPI backend are all defined using SQLAlchemy. The table models are stored in backend/app/db/models.py . Alembic \u00b6 Creating revisions \u00b6 The models are applied to the database using Alembic . Whenever you make any changes to the database models, you will need to generate a new Alembic \"revision\". You can use the bin/db-revision.sh script to help create a new revision after you've made some changes to the database models: bin/db-revision.sh \"Some short note\" This script uses Alembic's \"autogenerate\" feature to create what it thinks is the correct database migration script for the changes you made. Autogenerate is not always perfect , so you should always verify the migration script it creates before committing it to the repo (and applying it in production) to make sure what will be applied to the database is correct. The migration scripts can be found in backend/db/migrations/versions/ . An example migration script for creating the \"tag\" database table is shown below: \"\"\"Initial revision Revision ID: 03fe4895893f Revises: Create Date: 2021-04-16 21:08:29.237142 \"\"\" from alembic import op import sqlalchemy as sa # revision identifiers, used by Alembic revision = '03fe4895893f' down_revision = None branch_labels = None depends_on = None def upgrade () -> None : # ### commands auto generated by Alembic - please adjust! ### op . create_table ( 'tag' , sa . Column ( 'id' , sa . Integer (), nullable = False ), sa . Column ( 'name' , sa . String (), nullable = True ), sa . PrimaryKeyConstraint ( 'id' ) ) op . create_index ( op . f ( 'ix_tag_id' ), 'tag' , [ 'id' ], unique = False ) # ### end Alembic commands ### def downgrade () -> None : # ### commands auto generated by Alembic - please adjust! ### op . drop_index ( op . f ( 'ix_tag_id' ), table_name = 'tag' ) op . drop_table ( 'tag' ) # ### end Alembic commands ### This script shows you what it will do when you issue the \"upgrade\" command to the database as well as if you need to revert and issue the \"downgrade\" command. You can manually edit this migration script if something is incorrect. Applying revisions \u00b6 Once you have a new revision you'd like to apply to the database, you can either use the bin/reset-dev-container.sh script to rebuild your entire development environment (which will automatically apply the Alembic database revisions), or you can use the bin/db-upgrade.sh script to apply the revisions without erasing and rebuilding the development environment.","title":"Database"},{"location":"development/backend/database/#database","text":"","title":"Database"},{"location":"development/backend/database/#schema-definitions","text":"The database tables used by the FastAPI backend are all defined using SQLAlchemy. The table models are stored in backend/app/db/models.py .","title":"Schema definitions"},{"location":"development/backend/database/#alembic","text":"","title":"Alembic"},{"location":"development/backend/database/#creating-revisions","text":"The models are applied to the database using Alembic . Whenever you make any changes to the database models, you will need to generate a new Alembic \"revision\". You can use the bin/db-revision.sh script to help create a new revision after you've made some changes to the database models: bin/db-revision.sh \"Some short note\" This script uses Alembic's \"autogenerate\" feature to create what it thinks is the correct database migration script for the changes you made. Autogenerate is not always perfect , so you should always verify the migration script it creates before committing it to the repo (and applying it in production) to make sure what will be applied to the database is correct. The migration scripts can be found in backend/db/migrations/versions/ . An example migration script for creating the \"tag\" database table is shown below: \"\"\"Initial revision Revision ID: 03fe4895893f Revises: Create Date: 2021-04-16 21:08:29.237142 \"\"\" from alembic import op import sqlalchemy as sa # revision identifiers, used by Alembic revision = '03fe4895893f' down_revision = None branch_labels = None depends_on = None def upgrade () -> None : # ### commands auto generated by Alembic - please adjust! ### op . create_table ( 'tag' , sa . Column ( 'id' , sa . Integer (), nullable = False ), sa . Column ( 'name' , sa . String (), nullable = True ), sa . PrimaryKeyConstraint ( 'id' ) ) op . create_index ( op . f ( 'ix_tag_id' ), 'tag' , [ 'id' ], unique = False ) # ### end Alembic commands ### def downgrade () -> None : # ### commands auto generated by Alembic - please adjust! ### op . drop_index ( op . f ( 'ix_tag_id' ), table_name = 'tag' ) op . drop_table ( 'tag' ) # ### end Alembic commands ### This script shows you what it will do when you issue the \"upgrade\" command to the database as well as if you need to revert and issue the \"downgrade\" command. You can manually edit this migration script if something is incorrect.","title":"Creating revisions"},{"location":"development/backend/database/#applying-revisions","text":"Once you have a new revision you'd like to apply to the database, you can either use the bin/reset-dev-container.sh script to rebuild your entire development environment (which will automatically apply the Alembic database revisions), or you can use the bin/db-upgrade.sh script to apply the revisions without erasing and rebuilding the development environment.","title":"Applying revisions"},{"location":"development/backend/environment_variables/","text":"Environment Variables \u00b6 The backend container relies on a number of environment variables. For the development environment, these are automatically generated by the bin/reset-dev-container.sh script and stored inside of the .ace2.env file inside of your home directory. These environment variables will need to be set by other means if you are running this application in production. FastAPI backend variables \u00b6 These variables are used by the FastAPI backend application. ACE_DEV : If set (to anything), the application will run in development-mode, which means that the Alembic database migrations will be applied and the database seeded with basic information automatically when the application starts. This is enabled by default for the development environment. COOKIES_SAMESITE : The SameSite value to use when sending cookies. The development environment uses lax . Defaults to lax . COOKIES_SECURE : True/False whether or not you want to require HTTPS when sending cookies. The development environment uses False . Defaults to True . DATABASE_URL : The connection string used to connect to the PostgreSQL server. It should be in the form of postgresql://user:password@hostname[:port]/dbname . JWT_ACCESS_EXPIRE_SECONDS : The number of seconds after which an access token will expire. The development environment uses 900 (15 minutes) by default. JWT_ALGORITHM : Sets the algorithm to use for signing the tokens. The development environment uses HS256 by default. JWT_REFRESH_EXPIRE_SECONDS : The number of seconds after which a refresh token will expire. The development environment uses 43200 (12 hours) by default. JWT_SECRET : The secret key/password to use when signing and decoding tokens. The development environment generates a random 32 character string . SQL_ECHO : If set (to anything), SQLAlchemy will be configured to echo all queries to the console. You can view the queries in the Docker logs for the ace2-ams-api container. This is enabled by default for the development environment. PostgreSQL container variables \u00b6 These variables are used by the PostgreSQL server container to initialize the database. POSTGRES_DB : The name of the database to create. The development environment uses ace . POSTGRES_USER : The user to use to connect to the PostgreSQL server. The development environment uses ace . POSTGRES_PASSWORD : The password to use to connect to the PostgreSQL server. The development environment generates a random 32 character string .","title":"Environment Variables"},{"location":"development/backend/environment_variables/#environment-variables","text":"The backend container relies on a number of environment variables. For the development environment, these are automatically generated by the bin/reset-dev-container.sh script and stored inside of the .ace2.env file inside of your home directory. These environment variables will need to be set by other means if you are running this application in production.","title":"Environment Variables"},{"location":"development/backend/environment_variables/#fastapi-backend-variables","text":"These variables are used by the FastAPI backend application. ACE_DEV : If set (to anything), the application will run in development-mode, which means that the Alembic database migrations will be applied and the database seeded with basic information automatically when the application starts. This is enabled by default for the development environment. COOKIES_SAMESITE : The SameSite value to use when sending cookies. The development environment uses lax . Defaults to lax . COOKIES_SECURE : True/False whether or not you want to require HTTPS when sending cookies. The development environment uses False . Defaults to True . DATABASE_URL : The connection string used to connect to the PostgreSQL server. It should be in the form of postgresql://user:password@hostname[:port]/dbname . JWT_ACCESS_EXPIRE_SECONDS : The number of seconds after which an access token will expire. The development environment uses 900 (15 minutes) by default. JWT_ALGORITHM : Sets the algorithm to use for signing the tokens. The development environment uses HS256 by default. JWT_REFRESH_EXPIRE_SECONDS : The number of seconds after which a refresh token will expire. The development environment uses 43200 (12 hours) by default. JWT_SECRET : The secret key/password to use when signing and decoding tokens. The development environment generates a random 32 character string . SQL_ECHO : If set (to anything), SQLAlchemy will be configured to echo all queries to the console. You can view the queries in the Docker logs for the ace2-ams-api container. This is enabled by default for the development environment.","title":"FastAPI backend variables"},{"location":"development/backend/environment_variables/#postgresql-container-variables","text":"These variables are used by the PostgreSQL server container to initialize the database. POSTGRES_DB : The name of the database to create. The development environment uses ace . POSTGRES_USER : The user to use to connect to the PostgreSQL server. The development environment uses ace . POSTGRES_PASSWORD : The password to use to connect to the PostgreSQL server. The development environment generates a random 32 character string .","title":"PostgreSQL container variables"},{"location":"development/backend/insert_alerts/","text":"Insert Alerts \u00b6 There is a script at bin/insert-alerts.sh that allows you to insert alerts into the database that you then view in the GUI. Basic usage \u00b6 To insert a single alert into the database: bin/insert-alerts.sh backend/app/tests/alerts/alert.json You must pass a path to a JSON file that represents the structure of an alert's analysis and observable instance objects. There are already some example JSON files available: backend/app/tests/alerts/large.json backend/app/tests/alerts/small.json Adding multiple alerts \u00b6 You can use the script to add multiple alerts using the same JSON file. This will add 10 separate alerts to the database: bin/insert-alerts.sh backend/app/tests/alerts/small.json 10 Using a dynamic alert template \u00b6 You can also use an alert template where certain tokens inside of the JSON file get replaced with dynamic/random data. An example alert template is available at backend/app/tests/alerts/small_template.json . This template uses the following tokens that get replaced with random data using Faker: <ALERT_NAME> - A random name for the alert <A_TYPE> - Three random words used for the analysis module type <O_TYPE> - A random word used for the observable type <O_VALUE> - Two random words used for the observable value <TAG> - A random word used for a tag Using the dynamic template is the same command. To add 10 dynamic alerts to the database: bin/insert-alerts.sh backend/app/tests/alerts/small_template.json 10","title":"Insert Alerts"},{"location":"development/backend/insert_alerts/#insert-alerts","text":"There is a script at bin/insert-alerts.sh that allows you to insert alerts into the database that you then view in the GUI.","title":"Insert Alerts"},{"location":"development/backend/insert_alerts/#basic-usage","text":"To insert a single alert into the database: bin/insert-alerts.sh backend/app/tests/alerts/alert.json You must pass a path to a JSON file that represents the structure of an alert's analysis and observable instance objects. There are already some example JSON files available: backend/app/tests/alerts/large.json backend/app/tests/alerts/small.json","title":"Basic usage"},{"location":"development/backend/insert_alerts/#adding-multiple-alerts","text":"You can use the script to add multiple alerts using the same JSON file. This will add 10 separate alerts to the database: bin/insert-alerts.sh backend/app/tests/alerts/small.json 10","title":"Adding multiple alerts"},{"location":"development/backend/insert_alerts/#using-a-dynamic-alert-template","text":"You can also use an alert template where certain tokens inside of the JSON file get replaced with dynamic/random data. An example alert template is available at backend/app/tests/alerts/small_template.json . This template uses the following tokens that get replaced with random data using Faker: <ALERT_NAME> - A random name for the alert <A_TYPE> - Three random words used for the analysis module type <O_TYPE> - A random word used for the observable type <O_VALUE> - Two random words used for the observable value <TAG> - A random word used for a tag Using the dynamic template is the same command. To add 10 dynamic alerts to the database: bin/insert-alerts.sh backend/app/tests/alerts/small_template.json 10","title":"Using a dynamic alert template"},{"location":"usage/api/alert_filters/","text":"Alert Filters \u00b6 The API provides several ways to query for and filter alerts. Any of the filters can be combined with one another to produce complex queries. NOTE: If you combine multiple filters or use one of the filters that allows you to specify a comma-separated list of items, they are all queried for using AND logic. There is currently no way to perform OR or NOT logic with the filters, although that is planned for the future. Disposition \u00b6 To fetch all alerts that were assigned a disposition of DELIVERY : /api/alert/?disposition=DELIVERY To fetch all alerts that have not yet been dispositioned: /api/alert/?disposition=none Disposition User \u00b6 To fetch all alerts that were dispositioned by the username bob : /api/alert/?disposition_user=bob Dispositioned After \u00b6 To fetch all alerts that were dispositioned after January 1, 2021: /api/alert/?dispositioned_after=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded. Dispositioned Before \u00b6 To fetch all alerts that were dispositioned before January 1, 2021: /api/alert/?dispositioned_before=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded. Event UUID \u00b6 To fetch all alerts that are associated with the event UUID 98dab2bf-2683-48e7-8193-183c3c5e4490 : /api/alert/?event_uuid=98dab2bf-2683-48e7-8193-183c3c5e4490 Event Time After \u00b6 To fetch all alerts with the event_time after January 1, 2021: /api/alert/?event_time_after=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded. Event Time Before \u00b6 To fetch all alerts that were event_time before January 1, 2021: /api/alert/?event_time_before=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded. Insert Time After \u00b6 To fetch all alerts with the insert_time after January 1, 2021: /api/alert/?insert_time_after=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded. Insert Time Before \u00b6 To fetch all alerts that were insert_time before January 1, 2021: /api/alert/?insert_time_before=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded. Name \u00b6 To fetch all alerts with asdf in their name: /api/alert/?name=asdf NOTE: This performs a substring match. Observable \u00b6 To fetch all alerts that contain a specific observable with the type of fqdn and value of google.com : /api/alert/?observable=fqdn|google.com Observable Types \u00b6 To fetch all alerts that contain observables with the types of fqdn and ip : /api/alert/?observable_types=fqdn,ip Observable Value \u00b6 To fetch all alerts that contain observables (regardless of their type) that have the value google.com : /api/alert/?observable_value=google.com Owner \u00b6 To fetch all alerts that are owned by the username bob : /api/alert/?owner=bob Queue \u00b6 To fetch all alerts that are inside of the alert queue intel : /api/alert/?queue=intel Tags \u00b6 To fetch all alerts that have the tags email and malicious : /api/alert/?tags=email,malicious Threat Actor \u00b6 To fetch all alerts that were assigned the threat actor Bad Guy : /api/alert/?threat_actor=Bad Guy Threats \u00b6 To fetch all alerts that were assigned the threats emotet and zbot : /api/alert/?threats=emotet,zbot Tool \u00b6 To fetch all alerts produced by the tool Splunk : /api/alert/?tool=Splunk Tool Instance \u00b6 To fetch all alerts produced by the tool instance splunkserver1 : /api/alert/?tool_instance=splunkserver1 Type \u00b6 To fetch all alerts of type SMTP : /api/alert/?type=SMTP Combining timestamp filters \u00b6 You can combine the various timestamp filters to get alerts from a single day. For example, to get alerts that were dispositioned during November 1, 2021: /api/alert/?dispositioned_after=2021-11-01+00:00:00.000000+00:00&dispositioned_before=2021-11-02+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded.","title":"Alert Filters"},{"location":"usage/api/alert_filters/#alert-filters","text":"The API provides several ways to query for and filter alerts. Any of the filters can be combined with one another to produce complex queries. NOTE: If you combine multiple filters or use one of the filters that allows you to specify a comma-separated list of items, they are all queried for using AND logic. There is currently no way to perform OR or NOT logic with the filters, although that is planned for the future.","title":"Alert Filters"},{"location":"usage/api/alert_filters/#disposition","text":"To fetch all alerts that were assigned a disposition of DELIVERY : /api/alert/?disposition=DELIVERY To fetch all alerts that have not yet been dispositioned: /api/alert/?disposition=none","title":"Disposition"},{"location":"usage/api/alert_filters/#disposition-user","text":"To fetch all alerts that were dispositioned by the username bob : /api/alert/?disposition_user=bob","title":"Disposition User"},{"location":"usage/api/alert_filters/#dispositioned-after","text":"To fetch all alerts that were dispositioned after January 1, 2021: /api/alert/?dispositioned_after=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded.","title":"Dispositioned After"},{"location":"usage/api/alert_filters/#dispositioned-before","text":"To fetch all alerts that were dispositioned before January 1, 2021: /api/alert/?dispositioned_before=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded.","title":"Dispositioned Before"},{"location":"usage/api/alert_filters/#event-uuid","text":"To fetch all alerts that are associated with the event UUID 98dab2bf-2683-48e7-8193-183c3c5e4490 : /api/alert/?event_uuid=98dab2bf-2683-48e7-8193-183c3c5e4490","title":"Event UUID"},{"location":"usage/api/alert_filters/#event-time-after","text":"To fetch all alerts with the event_time after January 1, 2021: /api/alert/?event_time_after=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded.","title":"Event Time After"},{"location":"usage/api/alert_filters/#event-time-before","text":"To fetch all alerts that were event_time before January 1, 2021: /api/alert/?event_time_before=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded.","title":"Event Time Before"},{"location":"usage/api/alert_filters/#insert-time-after","text":"To fetch all alerts with the insert_time after January 1, 2021: /api/alert/?insert_time_after=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded.","title":"Insert Time After"},{"location":"usage/api/alert_filters/#insert-time-before","text":"To fetch all alerts that were insert_time before January 1, 2021: /api/alert/?insert_time_before=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded.","title":"Insert Time Before"},{"location":"usage/api/alert_filters/#name","text":"To fetch all alerts with asdf in their name: /api/alert/?name=asdf NOTE: This performs a substring match.","title":"Name"},{"location":"usage/api/alert_filters/#observable","text":"To fetch all alerts that contain a specific observable with the type of fqdn and value of google.com : /api/alert/?observable=fqdn|google.com","title":"Observable"},{"location":"usage/api/alert_filters/#observable-types","text":"To fetch all alerts that contain observables with the types of fqdn and ip : /api/alert/?observable_types=fqdn,ip","title":"Observable Types"},{"location":"usage/api/alert_filters/#observable-value","text":"To fetch all alerts that contain observables (regardless of their type) that have the value google.com : /api/alert/?observable_value=google.com","title":"Observable Value"},{"location":"usage/api/alert_filters/#owner","text":"To fetch all alerts that are owned by the username bob : /api/alert/?owner=bob","title":"Owner"},{"location":"usage/api/alert_filters/#queue","text":"To fetch all alerts that are inside of the alert queue intel : /api/alert/?queue=intel","title":"Queue"},{"location":"usage/api/alert_filters/#tags","text":"To fetch all alerts that have the tags email and malicious : /api/alert/?tags=email,malicious","title":"Tags"},{"location":"usage/api/alert_filters/#threat-actor","text":"To fetch all alerts that were assigned the threat actor Bad Guy : /api/alert/?threat_actor=Bad Guy","title":"Threat Actor"},{"location":"usage/api/alert_filters/#threats","text":"To fetch all alerts that were assigned the threats emotet and zbot : /api/alert/?threats=emotet,zbot","title":"Threats"},{"location":"usage/api/alert_filters/#tool","text":"To fetch all alerts produced by the tool Splunk : /api/alert/?tool=Splunk","title":"Tool"},{"location":"usage/api/alert_filters/#tool-instance","text":"To fetch all alerts produced by the tool instance splunkserver1 : /api/alert/?tool_instance=splunkserver1","title":"Tool Instance"},{"location":"usage/api/alert_filters/#type","text":"To fetch all alerts of type SMTP : /api/alert/?type=SMTP","title":"Type"},{"location":"usage/api/alert_filters/#combining-timestamp-filters","text":"You can combine the various timestamp filters to get alerts from a single day. For example, to get alerts that were dispositioned during November 1, 2021: /api/alert/?dispositioned_after=2021-11-01+00:00:00.000000+00:00&dispositioned_before=2021-11-02+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded.","title":"Combining timestamp filters"},{"location":"usage/api/alert_sorting/","text":"Alert Sorting \u00b6 The API provides several ways to sort the alerts that are returned by the /api/alert/ endpoint. Disposition \u00b6 To sort the alerts by their disposition: Ascending /api/alert/?sort=disposition|asc Descending /api/alert/?sort=disposition|desc NOTE: Sorting by disposition will be skipped if you are also filtering the alerts by disposition. Disposition Time \u00b6 To sort the alerts by their disposition time: Ascending /api/alert/?sort=disposition_time|asc Descending /api/alert/?sort=disposition_time|desc Disposition User \u00b6 To sort the alerts by their disposition user: Ascending /api/alert/?sort=disposition_user|asc Descending /api/alert/?sort=disposition_user|desc NOTE: Sorting by disposition user will be skipped if you are also filtering the alerts by disposition user. Event Time \u00b6 To sort the alerts by their event time: Ascending /api/alert/?sort=event_time|asc Descending /api/alert/?sort=event_time|desc Insert Time \u00b6 To sort the alerts by their insert time: Ascending /api/alert/?sort=insert_time|asc Descending /api/alert/?sort=insert_time|desc Name \u00b6 To sort the alerts by their name: Ascending /api/alert/?sort=name|asc Descending /api/alert/?sort=name|desc Owner \u00b6 To sort the alerts by their owner: Ascending /api/alert/?sort=owner|asc Descending /api/alert/?sort=owner|desc NOTE: Sorting by owner will be skipped if you are also filtering the alerts by owner. Queue \u00b6 To sort the alerts by their queue: Ascending /api/alert/?sort=queue|asc Descending /api/alert/?sort=queue|desc NOTE: Sorting by queue will be skipped if you are also filtering the alerts by queue. Type \u00b6 To sort the alerts by their type: Ascending /api/alert/?sort=type|asc Descending /api/alert/?sort=type|desc NOTE: Sorting by type will be skipped if you are also filtering the alerts by type.","title":"Alert Sorting"},{"location":"usage/api/alert_sorting/#alert-sorting","text":"The API provides several ways to sort the alerts that are returned by the /api/alert/ endpoint.","title":"Alert Sorting"},{"location":"usage/api/alert_sorting/#disposition","text":"To sort the alerts by their disposition: Ascending /api/alert/?sort=disposition|asc Descending /api/alert/?sort=disposition|desc NOTE: Sorting by disposition will be skipped if you are also filtering the alerts by disposition.","title":"Disposition"},{"location":"usage/api/alert_sorting/#disposition-time","text":"To sort the alerts by their disposition time: Ascending /api/alert/?sort=disposition_time|asc Descending /api/alert/?sort=disposition_time|desc","title":"Disposition Time"},{"location":"usage/api/alert_sorting/#disposition-user","text":"To sort the alerts by their disposition user: Ascending /api/alert/?sort=disposition_user|asc Descending /api/alert/?sort=disposition_user|desc NOTE: Sorting by disposition user will be skipped if you are also filtering the alerts by disposition user.","title":"Disposition User"},{"location":"usage/api/alert_sorting/#event-time","text":"To sort the alerts by their event time: Ascending /api/alert/?sort=event_time|asc Descending /api/alert/?sort=event_time|desc","title":"Event Time"},{"location":"usage/api/alert_sorting/#insert-time","text":"To sort the alerts by their insert time: Ascending /api/alert/?sort=insert_time|asc Descending /api/alert/?sort=insert_time|desc","title":"Insert Time"},{"location":"usage/api/alert_sorting/#name","text":"To sort the alerts by their name: Ascending /api/alert/?sort=name|asc Descending /api/alert/?sort=name|desc","title":"Name"},{"location":"usage/api/alert_sorting/#owner","text":"To sort the alerts by their owner: Ascending /api/alert/?sort=owner|asc Descending /api/alert/?sort=owner|desc NOTE: Sorting by owner will be skipped if you are also filtering the alerts by owner.","title":"Owner"},{"location":"usage/api/alert_sorting/#queue","text":"To sort the alerts by their queue: Ascending /api/alert/?sort=queue|asc Descending /api/alert/?sort=queue|desc NOTE: Sorting by queue will be skipped if you are also filtering the alerts by queue.","title":"Queue"},{"location":"usage/api/alert_sorting/#type","text":"To sort the alerts by their type: Ascending /api/alert/?sort=type|asc Descending /api/alert/?sort=type|desc NOTE: Sorting by type will be skipped if you are also filtering the alerts by type.","title":"Type"}]}